{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f57828-717b-42ff-95f7-ac4057d89bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:32:39.355 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.359 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.360 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.361 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.362 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-12-17 21:32:39.364 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.479 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\lizaa\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-17 21:32:39.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.482 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.490 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-17 21:32:39.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib,os\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import webbrowser\n",
    "from wordcloud import WordCloud\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Vectorizer\n",
    "news_vectorizer = open(\"models\\\\Vectorizer\", \"rb\")\n",
    "news_cv = joblib.load(news_vectorizer)\n",
    "\n",
    "#Loading Model\n",
    "def load_prediction_model(model):\n",
    "    loaded_model = joblib.load(open(os.path.join(model), \"rb\"))\n",
    "    return loaded_model\n",
    "\n",
    "# Get Category from Numeric Value\n",
    "def get_category(val, dict):\n",
    "    for key, value in dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "\n",
    "def add_parameter_ui(clf_name):\n",
    "    params={}\n",
    "    st.sidebar.write(\"Select values: \")\n",
    "\n",
    "    if clf_name == \"Logistic Regression\":\n",
    "        R = st.sidebar.slider(\"Regularization\",0.1,10.0,step=0.1)\n",
    "        MI = st.sidebar.slider(\"max_iter\",50,400,step=50)\n",
    "        params[\"R\"] = R\n",
    "        params[\"MI\"] = MI\n",
    "\n",
    "    elif clf_name == \"KNN\":\n",
    "        K = st.sidebar.slider(\"n_neighbors\",1,20)\n",
    "        params[\"K\"] = K\n",
    "\n",
    "    elif clf_name == \"SVM\":\n",
    "        C = st.sidebar.slider(\"Regularization\",0.01,10.0,step=0.01)\n",
    "        kernel = st.sidebar.selectbox(\"Kernel\",(\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"))\n",
    "        params[\"C\"] = C\n",
    "        params[\"kernel\"] = kernel\n",
    "\n",
    "    elif clf_name == \"Decision Tree\":\n",
    "        M = st.sidebar.slider(\"max_depth\", 2, 20)\n",
    "        C = st.sidebar.selectbox(\"Criterion\", (\"gini\", \"entropy\"))\n",
    "        SS = st.sidebar.slider(\"min_samples_split\",1,10)\n",
    "        params[\"M\"] = M\n",
    "        params[\"C\"] = C\n",
    "        params[\"SS\"] = SS\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_classifier(clf_name,params):\n",
    "    global clf\n",
    "    if clf_name == \"Logistic Regression\":\n",
    "        clf = LogisticRegression(C=params[\"R\"],max_iter=params[\"MI\"])\n",
    "\n",
    "    elif clf_name == \"KNN\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=params[\"K\"])\n",
    "\n",
    "    elif clf_name == \"SVM\":\n",
    "        clf = SVC(kernel=params[\"kernel\"],C=params[\"C\"])\n",
    "\n",
    "    elif clf_name == \"Decision Tree\":\n",
    "        clf = DecisionTreeClassifier(max_depth=params[\"M\"],criterion=params[\"C\"])\n",
    "\n",
    "    elif clf_name == \"Naive Bayes\":\n",
    "        clf = MultinomialNB()\n",
    "\n",
    "    return clf\n",
    "def process_text(text):\n",
    "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "    text = \" \".join(filtered_sentence)\n",
    "    return text\n",
    "\n",
    "def get_dataset():\n",
    "    data = pd.read_csv(\"data\\\\train.csv\")\n",
    "    data['News_length'] = data['content'].str.len()\n",
    "    data['Text_parsed'] = data['content'].apply(process_text)\n",
    "    label_encoder = preprocessing.LabelEncoder() \n",
    "    data['category']= label_encoder.fit_transform(data['category']) \n",
    "    return data\n",
    "\n",
    "\n",
    "#Plot Output\n",
    "def compute(Y_pred,Y_test):\n",
    "    # c1, c2 = st.beta_columns((4,3))\n",
    "    #Confusion Matrix\n",
    "    st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "    cm=confusion_matrix(Y_test,Y_pred)\n",
    "    class_label = [\"business\", \"tech\", \"politics\", \"sport\",\"entertainment\"]\n",
    "    df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n",
    "    plt.figure(figsize=(12, 7.5))\n",
    "    sns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\n",
    "    plt.title(\"Confusion Matrix\",fontsize=15)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    st.pyplot()\n",
    "    #Calculate Metrics\n",
    "    acc=accuracy_score(Y_test,Y_pred)\n",
    "    mse=mean_squared_error(Y_test,Y_pred)\n",
    "    precision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=1)\n",
    "    st.subheader(\"Metrics of the model: \")\n",
    "    st.text('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {} %\\nMean Squared Error: {}'.format(\n",
    "        precision,recall,fscore,acc*100, mse))\n",
    "\n",
    "\n",
    "\n",
    "#Build Model\n",
    "def model(clf):\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(data['Text_parsed'], \n",
    "                                                    data['category'],test_size=0.2,random_state=65)\n",
    "    ngram_range = (1,2)\n",
    "    min_df = 10\n",
    "    max_df = 1.\n",
    "    max_features = 300\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "    features_train = tfidf.fit_transform(X_train).toarray()\n",
    "    labels_train = Y_train\n",
    "    \n",
    "\n",
    "    features_test = tfidf.transform(X_test).toarray()\n",
    "    labels_test = Y_test\n",
    "    \n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "    Y_pred = clf.predict(features_test)\n",
    "    acc=accuracy_score(labels_test,Y_pred)\n",
    "    return clf, Y_test, Y_pred\n",
    "\n",
    "#tokenize for nlp\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def vec_for_learning(model_dbow, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model_dbow.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "data = get_dataset()\n",
    "X = data['Text_parsed']\n",
    "Y = data['category']\n",
    "\n",
    "def main():\n",
    "    activities = [\"About\",\"Data\", \"Prediction\",\"NLP\"]\n",
    "    choice = st.sidebar.selectbox(\"Choose Activity\", activities)\n",
    "    if choice==\"Data\":\n",
    "        st.title('Data')\n",
    "        st.write(\"The following is the DataFrame of the `BBC News` dataset.\")\n",
    "        data = pd.read_csv(\"data\\\\train.csv\")\n",
    "        st.write(data)\n",
    "    if choice==\"About\":\n",
    "        with st.container():\n",
    "            st.title(\"Welcome to News Classification ML App:wave:\")\n",
    "            st.markdown(\"![Web Application](https://i.gifer.com/991p.gif)\")\n",
    "            st.markdown(\"\"\" \n",
    "\t\t\t#### Built with Streamlit\n",
    "\t\t\t## By\n",
    "\t\t\t+ Devashree Pravakar\n",
    "\t\t\t\"\"\")\n",
    "            st.markdown(\"\"\"+ Arindam Rao\"\"\")\n",
    "            st.markdown(\"\"\"+ Kintali Pardha Saradhi\"\"\")\n",
    "            url = 'https://github.com/devashree1923/News-Classification'\n",
    "            if st.button('Github'):\n",
    "                webbrowser.open_new_tab(url)\n",
    "\n",
    "    if choice==\"Prediction\":\n",
    "        \n",
    "        st.info(\"Prediction with ML\")\n",
    "        news_text = st.text_area(\"Enter Text\", \"Type Here\")\n",
    "        all_ml_models = [\"Logistic Regression\", \"Naive Bayes\", \"Decision Tree\", \"SVM\", \"KNN\"]\n",
    "        model_choice = st.selectbox(\"Choose ML Model\", all_ml_models)\n",
    "        prediction_labels = {'business':0, 'tech':1, 'politics':2, 'sport':3, 'entertainment':4}\n",
    "        params = add_parameter_ui(model_choice)\n",
    "        \n",
    "        if st.button(\"Classify\"):\n",
    "            st.text(\"Original text ::\\n{}\".format(news_text))\n",
    "            news_text = process_text(news_text)\n",
    "            vect_text = news_cv.transform([news_text]).toarray()\n",
    "            clf = get_classifier(model_choice,params)\n",
    "            predictor, Y_pred,Y_test = model(clf)\n",
    "            prediction = predictor.predict(vect_text)\n",
    "            result = get_category(prediction, prediction_labels)\n",
    "            st.success(result)\n",
    "            st.markdown(\"<hr>\",unsafe_allow_html=True)\n",
    "            st.subheader(f\"Classifier Used: {model_choice}\")\n",
    "            compute(Y_pred,Y_test)\n",
    "            # if st.checkbox(\"WordCloud\"):\n",
    "            st.subheader(\"WordCloud: \")\n",
    "            c_text = news_text\n",
    "            wordcloud = WordCloud().generate(c_text)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            st.pyplot()\n",
    "    if choice==\"NLP\":\n",
    "        st.info(\"Natural Language Processing\")\n",
    "        news_text = st.text_area(\"Enter Text\", \"Type Here\")\n",
    "        c_text = news_text\n",
    "        df = pd.read_csv(\"data/Train_Processed.csv\")\n",
    "        if st.button(\"Classify\"):\n",
    "            prediction_labels = {0:'business', 1:'entertainment', 2:'politics', 3:'sport', 4:'tech'}\n",
    "            news_text = process_text(news_text)\n",
    "            news_text = pd.DataFrame({'Text':[news_text]})\n",
    "            train, test = train_test_split(df, test_size = 0.2, random_state=42)\n",
    "            news_text = news_text.apply(lambda r: TaggedDocument(words=tokenize_text(r['Text']), tags=[0]), axis=1)\n",
    "            test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['Text']), tags=[r.Category]), axis=1)\n",
    "            model_dbow = pickle.load(open('models\\\\nlp_model_dbow.sav', 'rb'))\n",
    "            model_logistic = pickle.load(open('models\\\\nlp_model.sav', 'rb'))\n",
    "            Y_text, X_text = vec_for_learning(model_dbow, news_text)\n",
    "            Y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "            Y_pred = model_logistic.predict(X_test)\n",
    "            Y_text = model_logistic.predict(X_text) \n",
    "            result = prediction_labels[Y_text[0]]\n",
    "            st.success(result)\n",
    "            st.markdown(\"<hr>\",unsafe_allow_html=True)\n",
    "            st.subheader(\"Classifier Used: NLP with logistic regression\")\n",
    "            compute(Y_pred, Y_test)\n",
    "            st.subheader(\"WordCloud: \")\n",
    "            \n",
    "            wordcloud = WordCloud().generate(c_text)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            st.pyplot()\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fce63b3a-1024-489a-8999-bb6fe07ac1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475df023-3717-4e67-96f6-1e72d15ca04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizaa\\NLP Project\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b878aeb-0d03-4e88-82ee-4224dcc1e426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
